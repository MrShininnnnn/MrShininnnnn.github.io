---
---

@inproceedings{shi-etal-2025-ualberta,
    selected=true,
    abbr={SemEval},
    url={https://aclanthology.org/2025.semeval-1.224/},
    pdf={https://aclanthology.org/2025.semeval-1.224.pdf},
    code={https://github.com/UAlberta-NLP/SemEval2025-EAMT},
    title = "{UA}lberta at {S}em{E}val-2025 Task 2: Prompting and Ensembling for Entity-Aware Translation",
    author = "Shi, Ning  and
      Basil, David  and
      Hauer, Bradley  and
      Nawal, Noshin  and
      Riley, Jai  and
      Teodorescu, Daniela  and
      Zhang, John  and
      Kondrak, Grzegorz",
    editor = "Rosenthal, Sara  and
      Ros{\'a}, Aiala  and
      Ghosh, Debanjan  and
      Zampieri, Marcos",
    booktitle = "Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    pages = "1709--1717",
    ISBN = "979-8-89176-273-2",
    abstract = "We describe the methods used by our UAlberta team for the SemEval-2025 Task 2 on Entity-Aware Machine Translation (EA-MT). Our methods leverage large language models with prompt engineering strategies suited to this task, including retrieval augmented generation and in-context learning. Our best results overall are obtained with ensembles of multiple models, leveraging named entity knowledge in the dataset. Finally, we provide proof-of-concept experiments showing that lexico-semantic knowledge can be used to identify high-quality translations. We further demonstrate that our methods can function even without gold named entity translations, by using an alternative knowledge base such as BabelNet."
}

@inproceedings{riley-etal-2025-semi,
    selected=false,
    abbr={COLING},
    url={https://aclanthology.org/2025.coling-main.419/},
    pdf={https://aclanthology.org/2025.coling-main.419.pdf},
    code={https://github.com/jai-riley/Sense-Projection},
    title = "Semi-Automated Construction of Sense-Annotated Datasets for Practically Any Language",
    author = "Riley, Jai  and
      Hauer, Bradley  and
      Hriti, Nafisa Sadaf  and
      Luo, Guoqing  and
      Mirzaei, Amirreza  and
      Rafiei, Ali  and
      Sheikhi, Hadi  and
      Siavashpour, Mahvash  and
      Tavakoli, Mohammad  and
      Shi, Ning  and
      Kondrak, Grzegorz",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    pages = "6270--6284",
    abstract = "High-quality sense-annotated datasets are vital for evaluating and comparing WSD systems. We present a novel approach to creating parallel sense-annotated datasets, which can be applied to any language that English can be translated into. The method incorporates machine translation, word alignment, sense projection, and sense filtering to produce silver annotations, which can then be revised manually to obtain gold datasets. By applying our method to Farsi, Chinese, and Bengali, we produce new parallel benchmark datasets, which are vetted by native speakers of each language. Our automatically-generated silver datasets are of higher quality than the annotations obtained with recent multilingual WSD systems, particularly on non-European languages."
}

@inproceedings{li-etal-2024-translation,
    selected=false,
    abbr={ACL},
    url={https://aclanthology.org/2024.acl-long.372/},
    pdf={https://aclanthology.org/2024.acl-long.372.pdf},
    code={https://github.com/UAlberta-NLP/KinshipAutoLex}, 
    title = "Translation-based Lexicalization Generation and Lexical Gap Detection: Application to Kinship Terms",
    author = "Li, Senyu  and
      Hauer, Bradley  and
      Shi, Ning  and
      Kondrak, Grzegorz",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.acl-long.372",
    pages = "6891--6900",
    abstract = "Constructing lexicons with explicitly identified lexical gaps is a vital part of building multilingual lexical resources. Prior work has leveraged bilingual dictionaries and linguistic typologies for semi-automatic identification of lexical gaps. Instead, we propose a generally-applicable algorithmic method to automatically generate concept lexicalizations, which is based on machine translation and hypernymy relations between concepts. The absence of a lexicalization implies a lexical gap. We apply our method to kinship terms, which make a suitable case study because of their explicit definitions and regular structure. Empirical evaluations demonstrate that our approach yields higher accuracy than BabelNet and ChatGPT. Our error analysis indicates that enhancing the quality of translations can further improve the accuracy of our method."
}

@inproceedings{shi-etal-2024-lexical,
    selected=true,
    abbr={*SEM},
    url={https://aclanthology.org/2024.starsem-1.10/},
    pdf={https://aclanthology.org/2024.starsem-1.10.pdf},
    code={https://github.com/ShiningLab/PromptSub},
    title = "Lexical Substitution as Causal Language Modeling",
    author = "Shi, Ning  and
      Hauer, Bradley  and
      Kondrak, Grzegorz",
    editor = "Bollegala, Danushka  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.starsem-1.10",
    pages = "120--132",
    abstract = "Causal language models such as the GPT series have achieved significant success across various domains. However, their application to the lexical substitution task (LST) remains largely unexplored due to inherent limitations in autoregressive decoding. Our work is motivated by our observation that existing LST approaches tend to suffer from a misalignment between the pre-training objectives of the language models that they employ, and their subsequent fine-tuning and application for substitute generation. We introduce PromptSub, the first system to use causal language modeling (CLM) for LST. Through prompt-aware fine-tuning, PromptSub not only enriches the given context with additional knowledge, but also leverages the unidirectional nature of autoregressive decoding. PromptSub consistently outperforms GeneSis, the best previously published supervised LST method. Further analysis demonstrates the potential of PromptSub to further benefit from increased model capacity, expanded data resources, and retrieval of external knowledge. By framing LST within the paradigm of CLM, our approach indicates the versatility of general CLM-based systems, such as ChatGPT, in catering to specialized tasks, including LST."
}

@inproceedings{shi-etal-2024-ualberta,
    selected=true,
    abbr={SemEval},
    url={https://aclanthology.org/2024.semeval-1.254/},
    pdf={https://aclanthology.org/2024.semeval-1.254.pdf},
    code={https://github.com/UAlberta-NLP/SemEval2024-STR},
    title = "{UA}lberta at {S}em{E}val-2024 Task 1: A Potpourri of Methods for Quantifying Multilingual Semantic Textual Relatedness and Similarity",
    author = "Shi, Ning  and
      Li, Senyu  and
      Luo, Guoqing  and
      Mirzaei, Amirreza  and
      Rafiei, Ali  and
      Riley, Jai  and
      Sheikhi, Hadi  and
      Siavashpour, Mahvash  and
      Tavakoli, Mohammad  and
      Hauer, Bradley  and
      Kondrak, Grzegorz",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Tayyar Madabushi, Harish  and
      Da San Martino, Giovanni  and
      Rosenthal, Sara  and
      Ros{\'a}, Aiala},
    booktitle = "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.semeval-1.254",
    pages = "1798--1805",
    abstract = "We describe our systems for SemEval-2024 Task 1: Semantic Textual Relatedness. We investigate the correlation between semantic relatedness and semantic similarity. Specifically, we test two hypotheses: (1) similarity is a special case of relatedness, and (2) semantic relatedness is preserved under translation. We experiment with a variety of approaches which are based on explicit semantics, downstream applications, contextual embeddings, large language models (LLMs), as well as ensembles of methods. We find empirical support for our theoretical insights. In addition, our best ensemble system yields highly competitive results in a number of diverse categories. Our code and data are available on GitHub."
}

@inproceedings{woudstra-etal-2024-identifying,
    selected=false,
    abbr={*SEM},
    url={https://aclanthology.org/2024.starsem-1.12/},
    pdf={https://aclanthology.org/2024.starsem-1.12.pdf},
    code={https://github.com/UAlberta-NLP/SentiSynset},
    title = "Identifying Emotional and Polar Concepts via Synset Translation",
    author = "Woudstra, Logan  and
      Dawodu, Moyo  and
      Igwe, Frances  and
      Li, Senyu  and
      Shi, Ning  and
      Hauer, Bradley  and
      Kondrak, Grzegorz",
    editor = "Bollegala, Danushka  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.starsem-1.12",
    pages = "142--152",
    abstract = "Emotion identification and polarity classification seek to determine the sentiment expressed by a writer. Sentiment lexicons that provide classifications at the word level fail to distinguish between different senses of polysemous words. To address this problem, we propose a translation-based method for labeling each individual lexical concept and word sense. Specifically, we translate synsets into 20 different languages and verify the sentiment of these translations in multilingual sentiment lexicons. By applying our method to all WordNet synsets, we produce SentiSynset, a synset-level sentiment resource containing 12,429 emotional synsets and 15,567 polar synsets, which is significantly larger than previous resources. Experimental evaluation shows that our method outperforms prior automated methods that classify word senses, in addition to outperforming ChatGPT. We make the resulting resource publicly available on GitHub."
}

@inproceedings{shi-etal-2024-paraphrase,
    selected=true,
    abbr={*SEM},
    url={https://aclanthology.org/2024.starsem-1.11/},
    pdf={https://aclanthology.org/2024.starsem-1.11.pdf},
    code={https://github.com/ShiningLab/PI2NLI}, 
    title = "Paraphrase Identification via Textual Inference",
    author = "Shi, Ning  and
      Hauer, Bradley  and
      Riley, Jai  and
      Kondrak, Grzegorz",
    editor = "Bollegala, Danushka  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.starsem-1.11",
    pages = "133--141",
    abstract = "Paraphrase identification (PI) and natural language inference (NLI) are two important tasks in natural language processing. Despite their distinct objectives, an underlying connection exists, which has been notably under-explored in empirical investigations. We formalize the relationship between these semantic tasks and introduce a method for solving PI using an NLI system, including the adaptation of PI datasets for fine-tuning NLI models. Through extensive evaluations on six PI benchmarks, across both zero-shot and fine-tuned settings, we showcase the efficacy of NLI models for PI through our proposed reduction. Remarkably, our fine-tuning procedure enables NLI models to outperform dedicated PI models on PI datasets. In addition, our findings provide insights into the limitations of current PI benchmarks."
}


@inproceedings{zhang-etal-2023-dont,
    selected=false,
    abbr={EMNLP},
    arxiv={2305.16339},
    url={https://aclanthology.org/2023.emnlp-main.491/},
    pdf={https://aclanthology.org/2023.emnlp-main.491.pdf},
    code={https://github.com/Senyu-Li/LLM-Multilingual-Types},
    title = "Don{'}t Trust {C}hat{GPT} when your Question is not in {E}nglish: A Study of Multilingual Abilities and Types of {LLM}s",
    author = "Zhang, Xiang  and
      Li, Senyu  and
      Hauer, Bradley  and
      Shi, Ning  and
      Kondrak, Grzegorz",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2023.emnlp-main.491",
    pages = "7915--7927",
    abstract = "Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing how they use LLMs and interpret their output. In this work, we propose a systematic way of qualitatively and quantitatively evaluating the multilingual capabilities of LLMs. We investigate the phenomenon of cross-language generalization in LLMs, wherein limited multilingual training data leads to advanced multilingual capabilities. To accomplish this, we employ a novel prompt back-translation method. The results demonstrate that LLMs, such as GPT, can effectively transfer learned knowledge across different languages, yielding relatively consistent results in translation-equivariant tasks, in which the correct output does not depend on the language of the input. However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers."
}

@inproceedings{ogezi-etal-2023-ualberta,
    selected=false, 
    abbr={SemEval}, 
    arxiv={2306.14067}, 
    pdf={https://aclanthology.org/2023.semeval-1.281.pdf}, 
    code={https://github.com/UAlberta-NLP/v-wsd}, 
    poster={UAlberta\ at\ SemEval\ 2023\ Task\ 1\ Context\ Augmentation\ and\ Translation\ for\ Visual\ WSD.pdf}, 
    slides={UAlberta\ at\ SemEval\ 2023\ Task\ 1\ Context\ Augmentation\ and\ Translation\ for\ Visual\ WSD.pdf}, 
    title = "{UA}lberta at {S}em{E}val-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation",
    author = "Ogezi, Michael  and
      Hauer, Bradley  and
      Omarov, Talgat  and
      Shi, Ning  and
      Kondrak, Grzegorz",
    booktitle = "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.semeval-1.281",
    pages = "2043--2051",
    abstract = "We describe the systems of the University of Alberta team for the SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel algorithm that leverages glosses retrieved from BabelNet, in combination with text and image encoders. Furthermore, we compare language-specific encoders against the application of English encoders to translated texts. As the contexts given in the task datasets are extremely short, we also experiment with augmenting these contexts with descriptions generated by a language model. This yields substantial improvements in accuracy. We describe and evaluate additional V-WSD methods which use image generation and text-conditioned image segmentation. Some of our experimental results exceed those of our official submissions on the test set. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd."
}

@inproceedings{chen-etal-2023-adversarial,
    selected=false, 
    abbr={ACL Findings}, 
    arxiv={2305.18503}, 
    pdf={https://aclanthology.org/2023.findings-acl.611.pdf}, 
    code={https://github.com/thunlp/RobTest}, 
    title = "From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework",
    author = "Chen, Yangyi  and
      Gao, Hongcheng  and
      Cui, Ganqu  and
      Yuan, Lifan  and
      Kong, Dehan  and
      Wu, Hanlu  and
      Shi, Ning  and
      Yuan, Bo  and
      Huang, Longtao  and
      Xue, Hui  and
      Liu, Zhiyuan  and
      Sun, Maosong  and
      Ji, Heng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.611",
    pages = "9607--9632",
    abstract = "Textual adversarial attacks can discover models{'} weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. To address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. Then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. Finally, we use the perturbation degree of adversarial samples to control the sample validity. We implement a toolkit \textbf{RobTest} that realizes our automatic robustness evaluation framework. In our experiments, we conduct a robustness evaluation of RoBERTa models to demonstrate the effectiveness of our evaluation framework, and further show the rationality of each component in the framework.",
}

@inproceedings{zhang-etal-2023-bridging,
    selected=false, 
    abbr={EACL}, 
    url={https://aclanthology.org/2023.eacl-main.205/}, 
    pdf={https://aclanthology.org/2023.eacl-main.205.pdf}, 
    code={https://github.com/senseAlign/BabelNet_2_HowNet}, 
    poster={Bridging\ the\ Gap\ Between\ BabelNet\ and\ HowNet\ Unsupervised\ Sense\ Alignment\ and\ Sememe\ Prediction.pdf}, 
    slides={Bridging\ the\ Gap\ Between\ BabelNet\ and\ HowNet\ Unsupervised\ Sense\ Alignment\ and\ Sememe\ Prediction.pdf}, 
    title = "Bridging the Gap Between {B}abel{N}et and {H}ow{N}et: Unsupervised Sense Alignment and Sememe Prediction",
    author = "Zhang, Xiang  and
      Shi, Ning  and
      Hauer, Bradley  and
      Kondrak, Grzegorz",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.205",
    pages = "2789--2798",
    abstract = "As the minimum semantic units of natural languages, sememes can provide precise representations of concepts. Despite the widespread utilization of lexical resources for semantic tasks, use of sememes is limited by a lack of available sememe knowledge bases. Recent efforts have been made to connect BabelNet with HowNet by automating sememe prediction. However, these methods depend on large manually annotated datasets. We propose to use sense alignment via a novel unsupervised and explainable method. Our method consists of four stages, each relaxing predefined constraints until a complete alignment of BabelNet synsets to HowNet senses is achieved. Experimental results demonstrate the superiority of our unsupervised method over previous supervised ones by an improvement of 12{\%} overall F1 score, setting a new state of the art. Our work is grounded in an interpretable propagation of sememe information between lexical resources, and may benefit downstream applications which can incorporate sememe information.",
}

@inproceedings{shi-etal-2022-revisit,
    selected=true, 
    abbr={BlackboxNLP}, 
    arxiv={2003.06658}, 
    url={https://aclanthology.org/2022.blackboxnlp-1.6/}, 
    pdf={https://aclanthology.org/2022.blackboxnlp-1.6.pdf}, 
    code={https://github.com/ShiningLab/Systematic-Generalization-via-Meaningful-Learning}, 
    poster={Revisit\ Systematic\ Generalization\ via\ Meaningful\ Learning.pdf}, 
    title = "Revisit Systematic Generalization via Meaningful Learning",
    author = "Shi, Ning  and
      Wang, Boxin  and
      Wang, Wei  and
      Liu, Xiangyu  and
      Lin, Zhouhan",
    booktitle = "Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.blackboxnlp-1.6",
    pages = "62--79",
    abstract = "Humans can systematically generalize to novel compositions of existing concepts. Recent studies argue that neural networks appear inherently ineffective in such cognitive capacity, leading to a pessimistic view and a lack of attention to optimistic results. We revisit this controversial topic from the perspective of meaningful learning, an exceptional capability of humans to learn novel concepts by connecting them with known ones. We reassess the compositional skills of sequence-to-sequence models conditioned on the semantic links between new and old concepts. Our observations suggest that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively. We demonstrate that prior knowledge plays a key role as well. In addition to synthetic tests, we further conduct proof-of-concept experiments in machine translation and semantic parsing, showing the benefits of meaningful learning in applications. We hope our positive findings will encourage excavating modern neural networks{'} potential in systematic generalization through more advanced learning schemes.",
}

@inproceedings{shi-etal-2022-text,
    selected=true, 
    abbr={EMNLP Findings}, 
    arxiv={2210.12276}, 
    url={https://aclanthology.org/2022.findings-emnlp.114/}, 
    pdf={https://aclanthology.org/2022.findings-emnlp.114.pdf}, 
    code={https://github.com/ShiningLab/Text-Editing-as-Imitation-Game}, 
    poster={Text\ Editing\ as\ Imitation\ Game.pdf}, 
    slides={Text\ Editing\ as\ Imitation\ Game.pdf}, 
    title = "Text Editing as Imitation Game",
    author = "Shi, Ning  and
      Tang, Bin  and
      Yuan, Bo  and
      Huang, Longtao  and
      Pu, Yewen  and
      Fu, Jie  and
      Lin, Zhouhan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.114",
    pages = "1583--1594",
    abstract = "Text editing, such as grammatical error correction, arises naturally from imperfect textual data. Recent works frame text editing as a multi-round sequence tagging task, where operations {--} such as insertion and substitution {--} are represented as a sequence of tags. While achieving good results, this encoding is limited in flexibility as all actions are bound to token-level tags. In this work, we reformulate text editing as an imitation game using behavioral cloning. Specifically, we convert conventional sequence-to-sequence data into state-to-action demonstrations, where the action space can be as flexible as needed. Instead of generating the actions one at a time, we introduce a dual decoders structure to parallel the decoding while retaining the dependencies between action tokens, coupled with trajectory augmentation to alleviate the distribution shift that imitation learning often suffers. In experiments on a suite of Arithmetic Equation benchmarks, our model consistently outperforms the autoregressive baselines in terms of performance, efficiency, and robustness. We hope our findings will shed light on future studies in reinforcement learning applying sequence-level action generation to natural language processing.",
}

@inproceedings{zhang-etal-2022-rochbert,
    selected=false, 
    abbr={EMNLP Findings}, 
    arxiv={2210.15944}, 
    url={https://aclanthology.org/2022.findings-emnlp.256/}, 
    pdf={https://aclanthology.org/2022.findings-emnlp.256.pdf}, 
    code={https://github.com/zzh-z/RoChBERT}, 
    title = "{R}o{C}h{B}ert: Towards Robust {BERT} Fine-tuning for {C}hinese",
    author = "Zhang, Zihan  and
      Li, Jinfeng  and
      Shi, Ning  and
      Yuan, Bo  and
      Liu, Xiangyu  and
      Zhang, Rong  and
      Xue, Hui  and
      Sun, Donghong  and
      Zhang, Chao",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.256",
    pages = "3502--3516",
    abstract = "Despite of the superb performance on a wide range of tasks, pre-trained language models (e.g., BERT) have been proved vulnerable to adversarial texts. In this paper, we present RoChBERT, a framework to build more Robust BERT-based models by utilizing a more comprehensive adversarial graph to fuse Chinese phonetic and glyph features into pre-trained representations during fine-tuning. Inspired by curriculum learning, we further propose to augment the training dataset with adversarial texts in combination with intermediate samples. Extensive experiments demonstrate that RoChBERT outperforms previous methods in significant ways: (i) robust {--} RoChBERT greatly improves the model robustness without sacrificing accuracy on benign texts. Specifically, the defense lowers the success rates of unlimited and limited attacks by 59.43{\%} and 39.33{\%} respectively, while remaining accuracy of 93.30{\%}; (ii) flexible {--} RoChBERT can easily extend to various language models to solve different downstream tasks with excellent performance; and (iii) efficient {--} RoChBERT can be directly applied to the fine-tuning stage without pre-training language model from scratch, and the proposed data augmentation method is also low-cost.",
}

@inproceedings{wang-etal-2021-counterfactual-adversarial,
    selected=false, 
    abbr={EMNLP Findings}, 
    arxiv={2109.04746}, 
    url={https://aclanthology.org/2021.findings-emnlp.413/}, 
    pdf={https://aclanthology.org/2021.findings-emnlp.413.pdf}, 
    code={https://github.com/ShiningLab/CAT}, 
    poster={Counterfactual\ Adversarial\ Learning\ with\ Representation\ Interpolation.pdf}, 
    slides={Counterfactual\ Adversarial\ Learning\ with\ Representation\ Interpolation.pdf}, 
    title = "Counterfactual Adversarial Learning with Representation Interpolation",
    author = "Wang, Wei  and
      Wang, Boxin  and
      Shi, Ning  and
      Li, Jinfeng  and
      Zhu, Bingyu  and
      Liu, Xiangyu  and
      Zhang, Rong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.413",
    doi = "10.18653/v1/2021.findings-emnlp.413",
    pages = "4809--4820",
    abstract = "Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering.",
}

@inproceedings{shi21_interspeech,
    selected=true, 
    abbr={INTERSPEECH}, 
    arxiv={2106.06731}, 
    pdf={https://www.isca-speech.org/archive/pdfs/interspeech_2021/shi21_interspeech.pdf}, 
    code={https://github.com/ShiningLab/POS-Tagger-for-Punctuation-Restoration}, 
    poster={Incorporating\ External\ POS\ Tagger\ for\ Punctuation\ Restoration.pdf}, 
    slides={Incorporating\ External\ POS\ Tagger\ for\ Punctuation\ Restoration.pdf}, 
    author={Ning Shi and Wei Wang and Boxin Wang and Jinfeng Li and Xiangyu Liu and Zhouhan Lin}, 
    title={{Incorporating External POS Tagger for Punctuation Restoration}}, 
    year=2021, 
    booktitle={Proc. Interspeech 2021}, 
    pages={1987--1991}, 
    url={https://www.isca-speech.org/archive/interspeech_2021/shi21_interspeech.html}, 
    doi={10.21437/Interspeech.2021-1708}, 
    abstract="Punctuation restoration is an important post-processing step in automatic speech recognition. Among other kinds of external information, part-of-speech (POS) taggers provide informative tags, suggesting each input token’s syntactic role, which has been shown to be beneficial for the punctuation restoration task. In this work, we incorporate an external POS tagger and fuse its predicted labels into the existing language model to provide syntactic information. Besides, we propose sequence boundary sampling (SBS) to learn punctuation positions more efficiently as a sequence tagging task. Experimental results show that our methods can consistently obtain performance gains and achieve a new state-of-the-art on the common IWSLT benchmark. Further ablation studies illustrate that both large pre-trained language models and the external POS tagger take essential parts to improve the model’s performance."
}

@inproceedings{shi-etal-2020-recurrent,
    selected=true, 
    abbr={EMNLP Findings}, 
    arxiv={2009.12643}, 
    pdf={https://aclanthology.org/2020.findings-emnlp.159.pdf}, 
    code={https://github.com/ShiningLab/Recurrent-Text-Editing}, 
    poster={Recurrent\ Inference\ in\ Text\ Editing.pdf}, 
    slides={Recurrent\ Inference\ in\ Text\ Editing.pdf}, 
    title = "Recurrent Inference in Text Editing",
    author = "Shi, Ning  and
      Zeng, Ziheng  and
      Zhang, Haotian  and
      Gong, Yichen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.159",
    doi = "10.18653/v1/2020.findings-emnlp.159",
    pages = "1758--1769",
    abstract = "In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation Simplification (AES), Arithmetic Equation Correction (AEC). Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.",
}
